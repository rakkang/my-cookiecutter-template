"""Generated by Cursor"""
{%- if cookiecutter.use_langchain == "yes" %}
from typing import Dict, Any, Generator, Callable, Type, List, Optional, Union
from functools import wraps
import time
from abc import ABC, abstractmethod
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, BaseMessage, AIMessage
from langchain_openai import OpenAIEmbeddings
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel
from ...utils.logger import logger

from ...config.env_config import ChatConfig as chat_config, EmbeddingConfig as embedding_config


def retry_on_rate_limit(max_retries: int = 3, initial_delay: float = 1):
    """
    重试装饰器，处理API的限流错误
    特别处理 HTTP 429 错误，使用更长的退避时间
    """
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            delay = initial_delay
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries:
                        raise e
                    
                    # 检查是否是 HTTP 429 错误（限流错误）
                    error_str = str(e).lower()
                    is_rate_limit = (
                        '429' in error_str or 
                        'too many requests' in error_str or
                        'rate limit' in error_str
                    )
                    
                    # 如果是 429 错误，使用更长的退避时间
                    if is_rate_limit:
                        # 429 错误需要更长的等待时间，使用指数退避但最小延迟更长
                        wait_time = max(delay * 2, 5.0)  # 至少等待5秒
                        logger.warning(f"遇到限流错误 (429)，等待 {wait_time:.1f} 秒后重试 (尝试 {attempt + 1}/{max_retries + 1})")
                    else:
                        wait_time = delay
                        logger.warning(f"请求失败，等待 {wait_time:.1f} 秒后重试 (尝试 {attempt + 1}/{max_retries + 1}): {str(e)[:100]}")
                    
                    time.sleep(wait_time)
                    delay *= 2
            return None
        return wrapper
    return decorator


class BaseLLMExecutor(ABC):
    """
    LLM执行器的抽象基类，定义了与语言模型交互的基本接口
    """
    
    def __init__(self, default_config: Optional[Dict[str, Any]] = None):
        """
        初始化执行器
        
        Args:
            default_config: 默认配置字典
        """
        self.default_config = default_config or {}

    def _get_chat_model(self, config: Optional[Dict[str, Any]] = None) -> ChatOpenAI:
        """
        获取ChatOpenAI模型实例
        """
        merged_config = {**self.default_config, **(config or {})}
        
        return ChatOpenAI(
            api_key=merged_config.get('api_key'),
            base_url=merged_config.get('base_url'),
            model=merged_config.get('model') or chat_config.MODEL_NAME,
            temperature=merged_config.get('temperature', 0.1),  # 为严肃场景设置低温度值
            top_p=merged_config.get('top_p', 0.1),  # 使用低top_p值增强确定性
            frequency_penalty=merged_config.get('frequency_penalty', 0.0),  # 防止重复
            presence_penalty=merged_config.get('presence_penalty', 0.0)  # 控制话题多样性
        )

    @abstractmethod
    def get_system_prompt(self) -> List[BaseMessage]:
        """
        获取基础消息列表，由子类实现以提供系统消息等
        
        Returns:
            消息列表
        """
        pass

    def _prepare_messages(self, message: str, history: Optional[List[BaseMessage]] = None) -> List[BaseMessage]:
        """
        准备完整的消息列表
        """
        messages = history or []
        messages.extend(self.get_system_prompt())
        if message:  # Only append if message is not empty or None
            messages.append(HumanMessage(content=message))
        return messages

    @retry_on_rate_limit()
    def run(self, message: str, config: Optional[Dict[str, Any]] = None, history: Optional[List[BaseMessage]] = None) -> Union[str, List[Union[str, Dict[str, Any]]]]:
        """
        执行单次对话并返回结果
        """
        default_chat_config = {
            'api_key': chat_config.OPENAI_API_KEY,
            'base_url': chat_config.OPENAI_API_BASE,
            'model': chat_config.MODEL_NAME,
            'max_tokens': chat_config.MAX_TOKENS,
            'temperature': 0.1,  # 为严肃场景设置低温度值
            'top_p': 0.1,  # 控制采样范围，低值增强确定性
            'frequency_penalty': 0.0,  # 不惩罚重复，保持输出一致性
            'presence_penalty': 0.0  # 不鼓励新话题，保持专注于当前主题
        }
        merged_config = {**default_chat_config, **self.default_config, **(config or {})}
        messages = self._prepare_messages(message, history)
        model = self._get_chat_model(merged_config)
        response = model.invoke(messages)
        return response.content

    @retry_on_rate_limit()
    def stream(self, message: str, config: Optional[Dict[str, Any]] = None, history: Optional[List[BaseMessage]] = None) -> Generator[Union[str, List[Union[str, Dict[str, Any]]]], None, None]:
        """
        执行流式对话并返回生成器
        """
        default_chat_config = {
            'api_key': chat_config.OPENAI_API_KEY,
            'base_url': chat_config.OPENAI_API_BASE,
            'model': chat_config.MODEL_NAME,
            'max_tokens': chat_config.MAX_TOKENS,
            'temperature': 0.1,  # 为严肃场景设置低温度值
            'top_p': 0.1,  # 低top_p值增强确定性
            'frequency_penalty': 0.0,  # 不惩罚重复词
            'presence_penalty': 0.0  # 不鼓励模型引入新话题
        }
        merged_config = {**default_chat_config, **self.default_config, **(config or {})}
        messages = self._prepare_messages(message, history)
        model = self._get_chat_model(merged_config)
        for chunk in model.stream(messages):
            if chunk.content:
                yield chunk.content

    @retry_on_rate_limit(max_retries=5, initial_delay=2)
    def structured(self, message: str, output_model: Type[BaseModel], config: Optional[Dict[str, Any]] = None, history: Optional[List[BaseMessage]] = None) -> BaseModel:
        """
        执行结构化对话并返回指定类型的结果
        """
        default_chat_config = {
            'api_key': chat_config.OPENAI_API_KEY,
            'base_url': chat_config.OPENAI_API_BASE,
            'model': chat_config.MODEL_NAME,
            'max_tokens': chat_config.MAX_TOKENS,
            'temperature': 0.1,  # 为严肃场景设置低温度值，提高结构化输出的一致性和准确性
            'top_p': 0.1,  # 控制输出的确定性，结构化输出中尤为重要
            'frequency_penalty': 0.0,  # 对于结构化输出，不惩罚重复词
            'presence_penalty': 0.0  # 对于结构化输出，保持模型对结构的关注
        }
        merged_config = {**default_chat_config, **self.default_config, **(config or {})}
        parser = PydanticOutputParser(pydantic_object=output_model)
        
        messages = self._prepare_messages(message, history)
        # 在系统消息中添加输出格式说明
        # 查找现有的 SystemMessage 并附加，或者如果没有，则预置一个新的 SystemMessage
        system_message_exists = False
        for msg in messages:
            if isinstance(msg, SystemMessage):
                msg.content = f"{msg.content}\n\n{parser.get_format_instructions()}"
                system_message_exists = True
                break
        if not system_message_exists:
            # 如果 history 可能为空，_get_messages() 可能也不返回 SystemMessage
            # 因此，我们需要一种更可靠的方式来确保 SystemMessage 存在或被创建
            # 如果 self._get_messages() 返回的第一个是 SystemMessage，则附加到那里，否则预置新的
            base_messages_from_get = self.get_system_prompt() # 调用一次以避免重复逻辑
            processed_messages = history or []
            
            system_message_in_base = False
            if base_messages_from_get:
                if isinstance(base_messages_from_get[0], SystemMessage):
                    base_messages_from_get[0].content = f"{base_messages_from_get[0].content}\n\n{parser.get_format_instructions()}"
                    system_message_in_base = True
            
            processed_messages.extend(base_messages_from_get)

            if not system_message_in_base and not any(isinstance(m, SystemMessage) for m in processed_messages):
                 # 只有在 history 和 _get_messages() 都没有系统消息时才添加
                processed_messages.insert(0, SystemMessage(content=parser.get_format_instructions()))
            
            processed_messages.append(HumanMessage(content=message))
            messages = processed_messages


        model = self._get_chat_model(merged_config)
        response = model.invoke(messages)

        # 记录消息为JSON格式
        try:
            # 将消息转换为可序列化的格式
            serializable_messages = []
            for msg in messages:
                serializable_messages.append({
                    "role": msg.type,
                    "content": msg.content
                })
            
            # logger.info(f"结构化对话消息: {json.dumps(serializable_messages, ensure_ascii=False)}")
            # logger.info(f"结构化对话响应: {response.content}")
        except Exception as e:
            logger.error(f"记录消息时出错: {str(e)}")

        content = response.content
        content_str = content if isinstance(content, str) else str(content) if content else ""
        return parser.parse(content_str)

    @retry_on_rate_limit()
    def create_embedding(self, text: str, config: Optional[Dict[str, Any]] = None) -> List[float]:
        """
        创建文本嵌入
        """
        default_embedding_config = {
            'api_key': embedding_config.OPENAI_API_KEY,
            'base_url': embedding_config.OPENAI_API_BASE,
            'model': embedding_config.EMBEDDING_MODEL
        }
        merged_config = {**default_embedding_config, **self.default_config, **(config or {})}
        embeddings = OpenAIEmbeddings(
            api_key=merged_config.get('api_key'),
            base_url=merged_config.get('base_url'),
            model=merged_config.get('model') or embedding_config.EMBEDDING_MODEL
        )
        return embeddings.embed_query(text)

    def convert_strings_to_messages(self, texts: List[str], mode: str = "user") -> List[BaseMessage]:
        """
        Generated by Cursor
        将字符串列表转换为BaseMessage对象列表。

        Args:
            texts: 要转换的字符串列表。
            mode: 转换模式。可以是 "user", "ai", 或 "alternate"。
                  - "user": 所有文本转换为HumanMessage。
                  - "ai": 所有文本转换为AIMessage。
                  - "alternate": 文本交替转换为HumanMessage和AIMessage，以HumanMessage开头。

        Returns:
            转换后的BaseMessage对象列表。

        Raises:
            ValueError: 如果提供了无效的模式。
        """
        messages: List[BaseMessage] = []
        if mode == "user":
            for text in texts:
                messages.append(HumanMessage(content=text))
        elif mode == "ai":
            for text in texts:
                messages.append(AIMessage(content=text))
        elif mode == "alternate":
            for i, text in enumerate(texts):
                if i % 2 == 0:
                    messages.append(HumanMessage(content=text))
                else:
                    messages.append(AIMessage(content=text))
        else:
            raise ValueError(f"无效的模式: {mode}. 可选模式为 'user', 'ai', 'alternate'.")
        return messages
{%- else %}
# base_llm_executor.py 仅在 use_langchain=yes 时使用
# 如果未启用 LangChain，此文件将在后处理钩子中被删除
pass
{%- endif %}
