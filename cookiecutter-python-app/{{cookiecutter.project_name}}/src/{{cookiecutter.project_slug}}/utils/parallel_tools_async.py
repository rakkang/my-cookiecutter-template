"""Generated by Cursor"""
{%- if cookiecutter.use_langchain == "yes" %}
from typing import List, Callable, TypeVar, Any, Optional, AsyncGenerator, Awaitable, Union, cast
import time
from functools import wraps
import asyncio
from enum import Enum
from .logger import logger
import psutil
from concurrent.futures import ThreadPoolExecutor

{%- if cookiecutter.use_fastapi == "yes" %}
from ..model.sse_message import SSEMessage, create_stage_progress
{%- endif %}

T = TypeVar('T')
R = TypeVar('R')

def _flush_log_handlers() -> None:
    """Generated by Cursor
    刷新所有日志处理器，确保控制台/文件实时写出
    """
    for handler in logger.handlers:
        try:
            handler.flush()
        except Exception:
            # 刷新失败不应中断主流程
            pass


class ExecutorType(Enum):
    """执行器类型枚举"""
    THREAD = "thread"


class AsyncParallelTools:
    """异步并行任务处理工具类（使用 asyncio）"""
    
    def __init__(self, max_workers: int = 5):
        """
        初始化异步并行工具
        
        Args:
            max_workers: 最大并发任务数（asyncio 中实际并发数可以更高）
        """
        self.max_workers = max_workers
        self.executor_type = ExecutorType.THREAD  # 保持兼容性
        self._rate_limit_lock = asyncio.Lock()
        self._request_times = []  # 用于滑动窗口限流
        # 线程池用于承载同步任务，防止阻塞事件循环
        self._executor: ThreadPoolExecutor = ThreadPoolExecutor(max_workers=self.max_workers)
        
        # 根据系统资源调整默认值
        if max_workers == 5:
            cpu_count = psutil.cpu_count(logical=False) or 4
            self.max_workers = min(max(2, cpu_count), 8)
        
        logger.info(f"AsyncParallelTools初始化: max_workers={self.max_workers}, "
                   f"executor_type=asyncio")

    async def _run_item(self,
                        item: T,
                        item_index: int,
                        process_func: Union[Callable[[T], R], Callable[[T], Awaitable[R]]],
                        rpm: int) -> Optional[R]:
        """
        Generated by Cursor
        统一的单项执行逻辑：限流 + 同/异步适配 + 日志刷新
        由批量与流式两处复用，避免重复代码。
        """
        if item is None:
            logger.warning("检测到空项目，跳过处理")
            _flush_log_handlers()
            return None
        
        try:
            await self._rate_limit(rpm)
            start_time = time.time()
            
            if asyncio.iscoroutinefunction(process_func):
                result = await process_func(item)
            else:
                loop = asyncio.get_running_loop()
                result = await loop.run_in_executor(self._executor, lambda: process_func(item))
            
            _ = time.time() - start_time  # 占位，保留耗时变量以便后续可能的指标采集
            # 及时把内部日志刷出，保证"看得到在动"
            _flush_log_handlers()
            return cast(Optional[R], result)
        except Exception:
            # 同样确保日志及时落盘，然后把异常交由上层策略处理（批量或流式分别处理）
            _flush_log_handlers()
            raise

    async def _rate_limit(self, rpm: int = 0):
        """
        实现请求频率限制（异步版本）- 使用滑动窗口算法，支持真正的并发
        
        Args:
            rpm: 每分钟最大请求数 (Requests Per Minute)，0表示不限制
        """
        if rpm <= 0:
            return
            
        current_time = time.time()
        
        async with self._rate_limit_lock:
            # 清理60秒之前的请求记录
            self._request_times = [t for t in self._request_times if current_time - t < 60.0]
            
            # 如果当前窗口内的请求数已达到限制，等待
            if len(self._request_times) >= rpm:
                # 计算需要等待的时间（直到最早的请求过期）
                oldest_request_time = min(self._request_times)
                wait_time = 60.0 - (current_time - oldest_request_time) + 0.01  # 加0.01秒缓冲
                if wait_time > 0:
                    await asyncio.sleep(wait_time)
                    current_time = time.time()
                    # 再次清理过期记录
                    self._request_times = [t for t in self._request_times if current_time - t < 60.0]
            
            # 记录本次请求时间
            self._request_times.append(current_time)

    def rate_limit_decorator(self, rpm: int = 0) -> Callable:
        """
        请求频率限制装饰器（异步版本）
        
        Args:
            rpm: 每分钟最大请求数，0表示不限制
        """
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            async def wrapper(*args, **kwargs):
                await self._rate_limit(rpm)
                if asyncio.iscoroutinefunction(func):
                    return await func(*args, **kwargs)
                else:
                    return func(*args, **kwargs)
            return wrapper
        return decorator

    async def process_batch(self,
                           items: List[T],
                           process_func: Union[Callable[[T], R], Callable[[T], Awaitable[R]]],
                           rpm: int = 0,
                           error_handler: Optional[Callable[[Exception], Any]] = None,
                           show_progress: bool = True,
                           timeout: float = 1800.0) -> List[R]:
        """
        并行处理批量任务（异步版本）
        
        Args:
            items: 待处理项目列表
            process_func: 处理单个项目的函数（可以是同步或异步）
            rpm: 每分钟最大请求数，0表示不限制
            error_handler: 错误处理函数
            show_progress: 是否显示进度
            timeout: 整体任务执行的超时时间（秒），默认1800秒
            
        Returns:
            处理结果列表
        """
        if not items:
            logger.warning("没有项目需要处理")
            return []
            
        total_items = len(items)
        logger.info(f"开始异步批量处理 {total_items} 个项目，设置超时时间: {timeout}秒")

        async def wrapped_process_func(item: T, item_index: int) -> Optional[R]:
            try:
                result = await self._run_item(item, item_index, process_func, rpm)
                return result
            except Exception as e:
                if error_handler:
                    if asyncio.iscoroutinefunction(error_handler):
                        return await error_handler(e)
                    else:
                        return error_handler(e)
                logger.error(f"处理任务时出错: {str(e)}")
                return None

        return await self._execute_with_asyncio(
            items, wrapped_process_func, show_progress, timeout
        )

    async def _execute_with_asyncio(self,
                                   items: List[T],
                                   wrapped_func: Callable,
                                   show_progress: bool,
                                   timeout: float) -> List[R]:
        """异步执行器执行逻辑 - 使用 asyncio"""
        # Generated by Cursor: 使用字典存储结果以保持顺序
        indexed_results = {}
        start_time = time.time()
        completed = 0
        total_items = len(items)
        
        try:
            # 创建所有任务并建立映射，改为"逐个完成处理"，实现实时进度
            tasks = []
            for idx, item in enumerate(items):
                task = asyncio.create_task(wrapped_func(item, idx))
                tasks.append((task, idx))
            task_to_idx = {task: idx for task, idx in tasks}
            pending_set = {task for task, _ in tasks}
            progress_interval = 0.5
            last_progress_time = time.time()
            
            while pending_set:
                # 使用 FIRST_COMPLETED 实时消费已完成任务，带超时保障
                wait_timeout = None
                if timeout is not None and timeout > 0:
                    remaining_time = timeout - (time.time() - start_time)
                    wait_timeout = max(progress_interval, 0.0) if remaining_time > 0 else 0.0
                done, pending = await asyncio.wait(
                    pending_set,
                    timeout=wait_timeout,
                    return_when=asyncio.FIRST_COMPLETED
                )
                
                # 如果整体超时，取消剩余任务
                if timeout and timeout > 0 and (time.time() - start_time) > timeout:
                    logger.error(f"异步批量处理超时，已完成 {completed}/{total_items} 项")
                    for t in pending_set:
                        t.cancel()
                    break
                
                pending_set = pending
                
                # 处理本轮完成的任务
                for t in done:
                    idx = task_to_idx.get(t)
                    if idx is None:
                        continue
                    try:
                        result = await t
                        indexed_results[idx] = result
                    except Exception as e:
                        logger.error(f"获取任务结果时出错: {str(e)}")
                        indexed_results[idx] = None
                    finally:
                        completed += 1
                        
                        # 按原策略每10%打印一次，也在时间间隔到达时补打一条，保证"看得到在动"
                        now = time.time()
                        if show_progress and (
                            completed % max(1, total_items // 10) == 0 or (now - last_progress_time) >= progress_interval
                        ):
                            progress_pct = (completed / total_items) * 100
                            elapsed = now - start_time
                            remaining = (elapsed / completed) * (total_items - completed) if completed > 0 else 0
                            logger.info(f"进度: {progress_pct:.1f}% ({completed}/{total_items}), "
                                        f"已用时间: {elapsed:.1f}秒, 预计剩余: {remaining:.1f}秒")
                            last_progress_time = now
            
            if pending:
                logger.error(f"异步批量处理超时，已完成 {completed}/{total_items} 项")
            
            if show_progress:
                elapsed = time.time() - start_time
                logger.info(f"处理完成，总耗时: {elapsed:.2f}秒")
                
        except Exception as e:
            logger.error(f"异步批量处理时发生错误: {str(e)}")
        
        # Generated by Cursor: 按原始顺序重新排列结果，保持所有位置
        results = []
        for i in range(len(items)):
            results.append(indexed_results.get(i, None))
            
        return results

{%- if cookiecutter.use_fastapi == "yes" %}
    async def process_batch_streaming(self,
                                     items: List[T],
                                     process_func: Union[Callable[[T], R], Callable[[T], Awaitable[R]]],
                                     stage: str = "processing",
                                     stage_name: str = "并行处理任务",
                                     rpm: int = 0,
                                     timeout: float = 1800.0) -> AsyncGenerator[SSEMessage, None]:
        """
        并行处理批量任务并以流式方式返回进度（异步版本）
        
        Args:
            items: 待处理项目列表
            process_func: 处理单个项目的函数（可以是同步或异步）
            stage: 处理阶段标识
            stage_name: 阶段名称
            rpm: 每分钟最大请求数，0表示不限制
            timeout: 整体任务执行的超时时间（秒），默认1800秒（30分钟）
            
        Yields:
            SSEMessage: 处理进度信息
        """
        if not items:
            logger.warning("没有项目需要处理")
            yield create_stage_progress(
                stage=stage,
                stage_name=stage_name,
                stage_progress=100,
                message="没有项目需要处理",
                overall_percent=100,
                status="success",
                is_end=True
            )
            return
            
        total_items = len(items)
        logger.info(f"开始异步批量处理 {total_items} 个项目，单任务超时时间: {timeout}秒")
        
        # 开始处理的进度信息
        yield create_stage_progress(
            stage=stage,
            stage_name=stage_name,
            stage_progress=0,
            message=f"开始处理{total_items}个项目 (使用 asyncio)",
            overall_percent=0,
            status="started"
        )

        try:
            async for msg in self._execute_streaming_with_asyncio(
                items, process_func, stage, stage_name, rpm, timeout
            ):
                yield msg
        except Exception as e:
            logger.error(f"异步流式批量处理时发生错误: {str(e)}")
            yield create_stage_progress(
                stage=stage,
                stage_name=stage_name,
                stage_progress=0,
                message=f"处理失败: {str(e)}",
                overall_percent=0,
                status="error",
                is_end=True
            )

    async def _execute_streaming_with_asyncio(self,
                                            items: List[T],
                                            process_func: Union[Callable[[T], R], Callable[[T], Awaitable[R]]],
                                            stage: str,
                                            stage_name: str,
                                            rpm: int,
                                            timeout: float) -> AsyncGenerator[SSEMessage, None]:
        """异步流式执行器执行逻辑"""
        # Generated by Cursor: 使用字典存储结果以保持顺序
        indexed_results = {}
        start_time = time.time()
        completed = 0
        last_progress_time = time.time()
        progress_interval = 0.5
        total_items = len(items)
        errors = []
        
        try:
            # 创建所有任务
            tasks = []
            for idx, item in enumerate(items):
                async def process_item(item: T, item_index: int) -> Optional[R]:
                    try:
                        return await self._run_item(item, item_index, process_func, rpm)
                    except Exception as e:
                        error_msg = f"处理项目 {item_index} 时出错: {str(e)}"
                        logger.error(error_msg)
                        _flush_log_handlers()
                        errors.append((item_index, error_msg))
                        return None
                
                task = asyncio.create_task(process_item(item, idx))
                tasks.append((task, idx))
            
            # 使用 asyncio.wait 等待任务完成
            processed_set = set()  # 已处理的任务集合
            pending_set = {task for task, _ in tasks}
            task_to_idx = {task: idx for task, idx in tasks}
            
            while pending_set:
                done, pending = await asyncio.wait(
                    pending_set,
                    timeout=min(progress_interval, timeout - (time.time() - start_time)) if timeout > 0 else progress_interval,
                    return_when=asyncio.FIRST_COMPLETED
                )
                
                pending_set = pending
                
                # 仅处理本轮完成的任务 - 避免每轮扫描全量任务
                for task in done:
                    if task not in processed_set:
                        idx = task_to_idx.get(task)
                        if idx is None:
                            continue
                        try:
                            result = await task
                            indexed_results[idx] = result
                            completed += 1
                            processed_set.add(task)
                            
                            current_time = time.time()
                            
                            # 在每个任务完成时即刻推送更细粒度的进度
                            progress_pct = int((completed / total_items) * 100)
                            elapsed = current_time - start_time
                            remaining = (elapsed / completed) * (total_items - completed) if completed > 0 else 0
                            
                            message = f"进度: {progress_pct}% ({completed}/{total_items})"
                            if remaining > 0:
                                message += f", 预计剩余: {remaining:.1f}秒"
                                
                            yield create_stage_progress(
                                stage=stage,
                                stage_name=stage_name,
                                stage_progress=progress_pct,
                                message=message,
                                overall_percent=progress_pct,
                                status="progress"
                            )
                            last_progress_time = current_time
                            
                            # 发送错误信息
                            if errors:
                                for item_idx, error_msg in errors:
                                    yield create_stage_progress(
                                        stage=stage,
                                        stage_name=stage_name,
                                        stage_progress=progress_pct,
                                        message=error_msg,
                                        overall_percent=progress_pct,
                                        status="progress",
                                        log_type="error"
                                    )
                                errors.clear()
                                
                        except Exception as e:
                            logger.error(f"获取任务结果时出错: {str(e)}")
                            indexed_results[idx] = None
                            processed_set.add(task)
                            
                            progress_pct = int((completed / total_items) * 100)
                            yield create_stage_progress(
                                stage=stage,
                                stage_name=stage_name,
                                stage_progress=progress_pct,
                                message=f"处理任务时出错: {str(e)}",
                                overall_percent=progress_pct,
                                status="progress",
                                log_type="error"
                            )
                
                # 检查超时
                if timeout > 0 and (time.time() - start_time) > timeout:
                    logger.error(f"异步流式处理超时，已完成 {completed}/{total_items} 项")
                    # 取消未完成的任务
                    for task in pending_set:
                        task.cancel()
                    break
            
            # 完成处理
            elapsed = time.time() - start_time
            logger.info(f"处理完成，总耗时: {elapsed:.2f}秒")
            
            # Generated by Cursor: 按原始顺序重新排列结果，保持所有位置
            results = []
            for i in range(len(items)):
                results.append(indexed_results.get(i, None))
            
            yield create_stage_progress(
                stage=stage,
                stage_name=stage_name,
                stage_progress=100,
                message=f"所有{total_items}个项目处理完成",
                overall_percent=100,
                status="success",
                result={"total_processed": completed, "total_items": total_items, "results": results}
            )
                
        except Exception as e:
            logger.error(f"异步批量处理时发生错误: {str(e)}")
{%- endif %}

    @staticmethod
    def chunk_list(lst: List[T], chunk_size: int) -> List[List[T]]:
        """将列表分割成固定大小的块"""
        return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]
{%- else %}
# parallel_tools_async.py 仅在 use_langchain=yes 时使用
# 如果未启用 LangChain，此文件将在后处理钩子中被删除
pass
{%- endif %}
